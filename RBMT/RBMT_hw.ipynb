{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Немецко-ивритский переводчик, основанный на правилах**\n",
    "\n",
    "Общий алгоритм выглядит следующим образом:\n",
    "1. В качестве инпута переводчику скармливается текст на немецком \n",
    "2. Модель осуществляет препроцессинг:\n",
    "\n",
    "    2.1. Сентенизацию (если текст состоит более, чем из одного предложения)\n",
    "\n",
    "    2.2. Очистку от знаков препинания\n",
    "\n",
    "    2.3. Токенизацию\n",
    "\n",
    "    2.4. Лемматизацию\n",
    "\n",
    "    2.5. POS-tagging и другие компоненты морфологической разметки\n",
    "\n",
    "    2.6. Синтаксический парсинг: извлечение информации о синтаксических зависимостях и порядке слов.\n",
    "    \n",
    "3. Из словаря выбираются лексические соответствия для языка-реципиента с учётом морфологических признаков (если, например, на вход подается предложение с перфектной формой вида haben/sein + PII, осуществляется выбор ивритской формы не только с учётом TAME-категорий, но и с учётом морфологического кодирования рода S-участника: Er hat gesehen -> הוא ראה [hu ra'ah], Sie hat gesehen -> היא ראתה [hi raatah])\n",
    "4. Переводчик применяет синтаксические правила языка-реципиента, формируя результат при помощи полученной информации о синтаксических зависимостях между лексическими единицами\n",
    "5. В качестве аутпута пользователь получает текст на иврите, в котором соблюдён, в частности, корректный порядок слов, правила которого так же обозначаются в структуре кода\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Библиотеки для препроцессинга текстов**\n",
    "\n",
    "В первую очередь, следует воспользоваться библиотекой stanza - для лемматизации и морфологической разметки. Выбор ее вместо привычных NLTK и SpaCy объясняется тем, что она предлагает модель для иврита. Библиотека предоставляет те же возможности, что и пакет UDPipe в R.\n",
    "\n",
    "Для начала нужно загрузить библиотеки и создать пайплайны для обоих языков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.9.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stanza) (4.25.5)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stanza) (3.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stanza) (2.4.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from stanza) (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.3.0->stanza) (1.13.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.3.0->stanza) (75.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->stanza) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->stanza) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Downloading stanza-1.9.2-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 17.4 MB/s eta 0:00:00\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   --------------------------------------- 586.9/586.9 kB 21.7 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.14.0 stanza-1.9.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 27.6MB/s]                    \n",
      "2024-12-04 02:36:35 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\resources.json\n",
      "2024-12-04 02:36:35 INFO: Downloading default packages for language: he (Hebrew) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-he/resolve/v1.9.0/models/default.zip: 100%|██████████| 346M/346M [00:11<00:00, 30.5MB/s] \n",
      "2024-12-04 02:36:48 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\he\\default.zip\n",
      "2024-12-04 02:36:50 INFO: Finished downloading models and saved to C:\\Users\\User\\stanza_resources\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 27.1MB/s]                    \n",
      "2024-12-04 02:36:50 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\resources.json\n",
      "2024-12-04 02:36:50 INFO: Downloading default packages for language: de (German) ...\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-de/resolve/v1.9.0/models/default.zip: 100%|██████████| 808M/808M [00:24<00:00, 32.7MB/s] \n",
      "2024-12-04 02:37:16 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\de\\default.zip\n",
      "2024-12-04 02:37:21 INFO: Finished downloading models and saved to C:\\Users\\User\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "import stanza \n",
    "\n",
    "stanza.download('he')\n",
    "stanza.download('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 02:41:17 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 25.2MB/s]                    \n",
      "2024-12-04 02:41:17 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\resources.json\n",
      "2024-12-04 02:41:18 INFO: Loading these models for language: de (German):\n",
      "===============================\n",
      "| Processor    | Package      |\n",
      "-------------------------------\n",
      "| tokenize     | gsd          |\n",
      "| mwt          | gsd          |\n",
      "| pos          | gsd_charlm   |\n",
      "| lemma        | gsd_nocharlm |\n",
      "| constituency | spmrl_charlm |\n",
      "| depparse     | gsd_charlm   |\n",
      "| sentiment    | sb10k_charlm |\n",
      "| ner          | germeval2014 |\n",
      "===============================\n",
      "\n",
      "2024-12-04 02:41:18 INFO: Using device: cpu\n",
      "2024-12-04 02:41:18 INFO: Loading: tokenize\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:19 INFO: Loading: mwt\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:19 INFO: Loading: pos\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:19 INFO: Loading: lemma\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:19 INFO: Loading: constituency\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\constituency\\base_trainer.py:87: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:20 INFO: Loading: depparse\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\depparse\\trainer.py:194: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:20 INFO: Loading: sentiment\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\classifiers\\trainer.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:20 INFO: Loading: ner\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\ner\\trainer.py:197: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:21 INFO: Done loading processors!\n",
      "2024-12-04 02:41:21 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json: 392kB [00:00, 26.3MB/s]                    \n",
      "2024-12-04 02:41:21 INFO: Downloaded file to C:\\Users\\User\\stanza_resources\\resources.json\n",
      "2024-12-04 02:41:21 INFO: Loading these models for language: he (Hebrew):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | iahlt_charlm      |\n",
      "=================================\n",
      "\n",
      "2024-12-04 02:41:21 INFO: Using device: cpu\n",
      "2024-12-04 02:41:21 INFO: Loading: tokenize\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-04 02:41:21 INFO: Loading: mwt\n",
      "2024-12-04 02:41:21 INFO: Loading: pos\n",
      "2024-12-04 02:41:22 INFO: Loading: lemma\n",
      "2024-12-04 02:41:22 INFO: Loading: depparse\n",
      "2024-12-04 02:41:22 INFO: Loading: ner\n",
      "2024-12-04 02:41:22 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "de = stanza.Pipeline('de')\n",
    "he = stanza.Pipeline('he')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользовавшись кодом ниже, ознакомимся с характеристиками лексем, которые есть в тексте инпута"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"id\": 1,\n",
      "  \"text\": \"Guten\",\n",
      "  \"lemma\": \"gut\",\n",
      "  \"upos\": \"ADJ\",\n",
      "  \"xpos\": \"ADJA\",\n",
      "  \"feats\": \"Case=Acc|Degree=Pos|Gender=Masc|Number=Sing\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"amod\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 5\n",
      "}\n",
      "{\n",
      "  \"id\": 2,\n",
      "  \"text\": \"Tag\",\n",
      "  \"lemma\": \"Tag\",\n",
      "  \"upos\": \"NOUN\",\n",
      "  \"xpos\": \"NN\",\n",
      "  \"feats\": \"Case=Acc|Gender=Masc|Number=Sing\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 6,\n",
      "  \"end_char\": 9\n",
      "}\n",
      "{\n",
      "  \"id\": 3,\n",
      "  \"text\": \"!\",\n",
      "  \"lemma\": \"!\",\n",
      "  \"upos\": \"PUNCT\",\n",
      "  \"xpos\": \"$.\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"punct\",\n",
      "  \"start_char\": 9,\n",
      "  \"end_char\": 10\n",
      "}\n",
      "{\n",
      "  \"id\": 1,\n",
      "  \"text\": \"Wie\",\n",
      "  \"lemma\": \"wie\",\n",
      "  \"upos\": \"ADV\",\n",
      "  \"xpos\": \"PWAV\",\n",
      "  \"feats\": \"PronType=Int\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"advmod\",\n",
      "  \"start_char\": 11,\n",
      "  \"end_char\": 14\n",
      "}\n",
      "{\n",
      "  \"id\": 2,\n",
      "  \"text\": \"heißt\",\n",
      "  \"lemma\": \"heißen\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VVFIN\",\n",
      "  \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 15,\n",
      "  \"end_char\": 20\n",
      "}\n",
      "{\n",
      "  \"id\": 3,\n",
      "  \"text\": \"du\",\n",
      "  \"lemma\": \"du\",\n",
      "  \"upos\": \"PRON\",\n",
      "  \"xpos\": \"PPER\",\n",
      "  \"feats\": \"Case=Nom|Number=Sing|Person=2|PronType=Prs\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"nsubj\",\n",
      "  \"start_char\": 21,\n",
      "  \"end_char\": 23\n",
      "}\n",
      "{\n",
      "  \"id\": 4,\n",
      "  \"text\": \"?\",\n",
      "  \"lemma\": \"?\",\n",
      "  \"upos\": \"PUNCT\",\n",
      "  \"xpos\": \"$.\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"punct\",\n",
      "  \"start_char\": 23,\n",
      "  \"end_char\": 24\n",
      "}\n",
      "{\n",
      "  \"id\": 1,\n",
      "  \"text\": \"Ich\",\n",
      "  \"lemma\": \"ich\",\n",
      "  \"upos\": \"PRON\",\n",
      "  \"xpos\": \"PPER\",\n",
      "  \"feats\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"nsubj\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 28\n",
      "}\n",
      "{\n",
      "  \"id\": 2,\n",
      "  \"text\": \"freue\",\n",
      "  \"lemma\": \"freuen\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VVFIN\",\n",
      "  \"feats\": \"Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\",\n",
      "  \"head\": 0,\n",
      "  \"deprel\": \"root\",\n",
      "  \"start_char\": 29,\n",
      "  \"end_char\": 34\n",
      "}\n",
      "{\n",
      "  \"id\": 3,\n",
      "  \"text\": \"mich\",\n",
      "  \"lemma\": \"ich\",\n",
      "  \"upos\": \"PRON\",\n",
      "  \"xpos\": \"PRF\",\n",
      "  \"feats\": \"Case=Acc|Number=Sing|Person=1|PronType=Prs|Reflex=Yes\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"start_char\": 35,\n",
      "  \"end_char\": 39\n",
      "}\n",
      "{\n",
      "  \"id\": 4,\n",
      "  \"text\": \",\",\n",
      "  \"lemma\": \",\",\n",
      "  \"upos\": \"PUNCT\",\n",
      "  \"xpos\": \"$,\",\n",
      "  \"head\": 6,\n",
      "  \"deprel\": \"punct\",\n",
      "  \"start_char\": 39,\n",
      "  \"end_char\": 40\n",
      "}\n",
      "{\n",
      "  \"id\": 5,\n",
      "  \"text\": \"dich\",\n",
      "  \"lemma\": \"du\",\n",
      "  \"upos\": \"PRON\",\n",
      "  \"xpos\": \"PPER\",\n",
      "  \"feats\": \"Case=Acc|Number=Sing|Person=1|PronType=Prs\",\n",
      "  \"head\": 6,\n",
      "  \"deprel\": \"obj\",\n",
      "  \"start_char\": 41,\n",
      "  \"end_char\": 45\n",
      "}\n",
      "{\n",
      "  \"id\": 6,\n",
      "  \"text\": \"kennenzulernen\",\n",
      "  \"lemma\": \"kennenlernen\",\n",
      "  \"upos\": \"VERB\",\n",
      "  \"xpos\": \"VVIZU\",\n",
      "  \"feats\": \"VerbForm=Inf\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"xcomp\",\n",
      "  \"start_char\": 46,\n",
      "  \"end_char\": 60\n",
      "}\n",
      "{\n",
      "  \"id\": 7,\n",
      "  \"text\": \".\",\n",
      "  \"lemma\": \".\",\n",
      "  \"upos\": \"PUNCT\",\n",
      "  \"xpos\": \"$.\",\n",
      "  \"head\": 2,\n",
      "  \"deprel\": \"punct\",\n",
      "  \"start_char\": 60,\n",
      "  \"end_char\": 61\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "text = 'Guten Tag! Wie heißt du? Ich freue mich, dich kennenzulernen.'\n",
    "doc = de(text)\n",
    "\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем нужные характеристики "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guten : gut, POS: ADJ, Gram: Case=Acc|Degree=Pos|Gender=Masc|Number=Sing, Syntax: amod\n",
      "tag : Tag, POS: NOUN, Gram: Case=Acc|Gender=Masc|Number=Sing, Syntax: root\n",
      "! : !, POS: PUNCT, Gram: None, Syntax: punct\n",
      "wie : wie, POS: ADV, Gram: PronType=Int, Syntax: advmod\n",
      "heißt : heißen, POS: VERB, Gram: Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin, Syntax: root\n",
      "du : du, POS: PRON, Gram: Case=Nom|Number=Sing|Person=2|PronType=Prs, Syntax: nsubj\n",
      "? : ?, POS: PUNCT, Gram: None, Syntax: punct\n",
      "ich : ich, POS: PRON, Gram: Case=Nom|Number=Sing|Person=1|PronType=Prs, Syntax: nsubj\n",
      "freue : freuen, POS: VERB, Gram: Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin, Syntax: root\n",
      "mich : ich, POS: PRON, Gram: Case=Acc|Number=Sing|Person=1|PronType=Prs|Reflex=Yes, Syntax: obj\n",
      ", : ,, POS: PUNCT, Gram: None, Syntax: punct\n",
      "dich : du, POS: PRON, Gram: Case=Acc|Number=Sing|Person=1|PronType=Prs, Syntax: obj\n",
      "kennenzulernen : kennenlernen, POS: VERB, Gram: VerbForm=Inf, Syntax: xcomp\n",
      ". : ., POS: PUNCT, Gram: None, Syntax: punct\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f'{word.text.lower()} : {word.lemma}, POS: {word.upos}, Gram: {word.feats}, Syntax: {word.deprel}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем то же самое с текстом на иврите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "יום : יום, POS: NOUN, Gram: Gender=Masc|Number=Sing, Syntax: obl:tmod\n",
      "טוב : טוב, POS: ADJ, Gram: Gender=Masc|Number=Sing, Syntax: amod\n",
      "! : !, POS: PUNCT, Gram: None, Syntax: punct\n",
      "איך : איך, POS: ADV, Gram: PronType=Int, Syntax: advmod\n",
      "קוראים : קרא, POS: VERB, Gram: Gender=Masc|HebBinyan=PAAL|Number=Plur|Person=3|Tense=Pres|VerbForm=Part|Voice=Act, Syntax: root\n",
      "ל : ל, POS: ADP, Gram: None, Syntax: case\n",
      "ך : הוא, POS: PRON, Gram: Gender=Masc|Number=Sing|Person=2|PronType=Prs, Syntax: obl\n",
      "? : ?, POS: PUNCT, Gram: None, Syntax: punct\n",
      "אני : הוא, POS: PRON, Gram: Gender=Masc|Number=Sing|Person=1|PronType=Prs, Syntax: nsubj\n",
      "שמח : שמח, POS: VERB, Gram: Gender=Masc|HebBinyan=PAAL|Number=Sing|Person=1|Tense=Pres|VerbForm=Part|Voice=Act, Syntax: root\n",
      "לפגוש : פגש, POS: VERB, Gram: HebBinyan=HIFIL|VerbForm=Inf|Voice=Act, Syntax: xcomp\n",
      "אות : את, POS: ADP, Gram: Case=Acc, Syntax: case\n",
      "ך : הוא, POS: PRON, Gram: Gender=Masc|Number=Sing|Person=2|PronType=Prs, Syntax: obj\n"
     ]
    }
   ],
   "source": [
    "text_1 = 'יום טוב! איך קוראים לך? אני שמח לפגוש אותך'\n",
    "doc_1 = he(text_1)\n",
    "\n",
    "for sent in doc_1.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f'{word.text.lower()} : {word.lemma}, POS: {word.upos}, Gram: {word.feats}, Syntax: {word.deprel}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы имеем набор лемм и грамматических признаков для лексем в составе двух эквивалентных предложений.\n",
    "Из них необходимо сформировать словарь, в котором будут представлены не только пословные соответствия, но и соответствия морфологических признаков и синтаксических ролей. Структура будет представлена в виде словаря словарей, где информация о грамматических признаках будет содержаться во внутреннем словаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_info = {\n",
    "    \"guten\": {\n",
    "        \"lemma\": \"gut\",\n",
    "        \"POS\": \"ADJ\",\n",
    "        \"Gram\": \"Case=Acc|Degree=Pos|Gender=Masc|Number=Sing\",\n",
    "        \"Syntax\": \"amod\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"טוב\",\n",
    "            \"lemma\": \"טוב\",\n",
    "            \"POS\": \"ADJ\",\n",
    "            \"Gram\": \"Gender=Masc|Number=Sing\",\n",
    "            \"Syntax\": \"amod\"\n",
    "        }\n",
    "    },\n",
    "    \"tag\": {\n",
    "        \"lemma\": \"Tag\",\n",
    "        \"POS\": \"NOUN\",\n",
    "        \"Gram\": \"Case=Acc|Gender=Masc|Number=Sing\",\n",
    "        \"Syntax\": \"root\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"יום\",\n",
    "            \"lemma\": \"יום\",\n",
    "            \"POS\": \"NOUN\",\n",
    "            \"Gram\": \"Gender=Masc|Number=Sing\",\n",
    "            \"Syntax\": \"root\"\n",
    "        }\n",
    "    },\n",
    "    \"!\": {\n",
    "        \"lemma\": \"!\",\n",
    "        \"POS\": \"PUNCT\",\n",
    "        \"Gram\": \"None\",\n",
    "        \"Syntax\": \"punct\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"!\",\n",
    "            \"lemma\": \"!\",\n",
    "            \"POS\": \"PUNCT\",\n",
    "            \"Gram\": \"None\",\n",
    "            \"Syntax\": \"punct\"\n",
    "        }\n",
    "    },\n",
    "    \"wie\": {\n",
    "        \"lemma\": \"wie\",\n",
    "        \"POS\": \"ADV\",\n",
    "        \"Gram\": \"PronType=Int\",\n",
    "        \"Syntax\": \"advmod\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"איך\",\n",
    "            \"lemma\": \"איך\",\n",
    "            \"POS\": \"ADV\",\n",
    "            \"Gram\": \"PronType=Int\",\n",
    "            \"Syntax\": \"advmod\"\n",
    "        }\n",
    "    },\n",
    "    \"heißt\": {\n",
    "        \"lemma\": \"heißen\",\n",
    "        \"POS\": \"VERB\",\n",
    "        \"Gram\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
    "        \"Syntax\": \"root\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"קוראים\",\n",
    "            \"lemma\": \"קרא\",\n",
    "            \"POS\": \"VERB\",\n",
    "            \"Gram\": \"Gender=Masc|HebBinyan=PAAL|Number=Plur|Person=3|Tense=Pres|VerbForm=Part|Voice=Act\",\n",
    "            \"Syntax\": \"root\"\n",
    "        }\n",
    "    },\n",
    "    \"du\": {\n",
    "        \"lemma\": \"du\",\n",
    "        \"POS\": \"PRON\",\n",
    "        \"Gram\": \"Case=Nom|Number=Sing|Person=2|PronType=Prs\",\n",
    "        \"Syntax\": \"nsubj\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"ך\",\n",
    "            \"lemma\": \"הוא\",\n",
    "            \"POS\": \"PRON\",\n",
    "            \"Gram\": \"Gender=Masc|Number=Sing|Person=2|PronType=Prs\",\n",
    "            \"Syntax\": \"obl\"\n",
    "        }\n",
    "    },\n",
    "    \"?\": {\n",
    "        \"lemma\": \"?\",\n",
    "        \"POS\": \"PUNCT\",\n",
    "        \"Gram\": \"None\",\n",
    "        \"Syntax\": \"punct\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"?\",\n",
    "            \"lemma\": \"?\",\n",
    "            \"POS\": \"PUNCT\",\n",
    "            \"Gram\": \"None\",\n",
    "            \"Syntax\": \"punct\"\n",
    "        }\n",
    "    },\n",
    "    \"ich\": {\n",
    "        \"lemma\": \"ich\",\n",
    "        \"POS\": \"PRON\",\n",
    "        \"Gram\": \"Case=Nom|Number=Sing|Person=1|PronType=Prs\",\n",
    "        \"Syntax\": \"nsubj\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"אני\",\n",
    "            \"lemma\": \"אני\",\n",
    "            \"POS\": \"PRON\",\n",
    "            \"Gram\": \"Gender=Masc|Number=Sing|Person=1|PronType=Prs\",\n",
    "            \"Syntax\": \"nsubj\"\n",
    "        }\n",
    "    },\n",
    "    \"freue\": {\n",
    "        \"lemma\": \"freuen\",\n",
    "        \"POS\": \"VERB\",\n",
    "        \"Gram\": \"Mood=Ind|Number=Sing|Person=1|Tense=Pres|VerbForm=Fin\",\n",
    "        \"Syntax\": \"root\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"שמח\",\n",
    "            \"lemma\": \"שמח\",\n",
    "            \"POS\": \"VERB\",\n",
    "            \"Gram\": \"Gender=Masc|HebBinyan=PAAL|Number=Sing|Person=1|Tense=Pres|VerbForm=Part|Voice=Act\",\n",
    "            \"Syntax\": \"root\"\n",
    "        }\n",
    "    },\n",
    "    \"mich\": {\n",
    "        \"lemma\": \"ich\",\n",
    "        \"POS \": \"PRON\",\n",
    "        \"Gram\": \"Case=Acc|Number=Sing|Person=1|PronType=Prs|Reflex=Yes\",\n",
    "        \"Syntax\": \"obj\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"אותך\",\n",
    "            \"lemma\": \"אותך\",\n",
    "            \"POS\": \"PRON\",\n",
    "            \"Gram\": \"Case=Acc|Number=Sing|Person=1|PronType=Prs|Reflex=Yes\",\n",
    "            \"Syntax\": \"obj\"\n",
    "        }\n",
    "    },\n",
    "    \",\": {\n",
    "        \"lemma\": \",\",\n",
    "        \"POS\": \"PUNCT\",\n",
    "        \"Gram\": \"None\",\n",
    "        \"Syntax\": \"punct\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \",\",\n",
    "            \"lemma\": \",\",\n",
    "            \"POS\": \"PUNCT\",\n",
    "            \"Gram\": \"None\",\n",
    "            \"Syntax\": \"punct\"\n",
    "        }\n",
    "    },\n",
    "    \"dich\": {\n",
    "        \"lemma\": \"du\",\n",
    "        \"POS\": \"PRON\",\n",
    "        \"Gram\": \"Case=Acc|Number=Sing|Person=1|PronType=Prs\",\n",
    "        \"Syntax\": \"obj\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"לך\",\n",
    "            \"lemma\": \"לך\",\n",
    "            \"POS\": \"PRON\",\n",
    "            \"Gram\": \"Case=Acc|Number=Sing|Person=2|PronType=Prs\",\n",
    "            \"Syntax\": \"obj\"\n",
    "        }\n",
    "    },\n",
    "    \"kennenzulernen\": {\n",
    "        \"lemma\": \"kennenlernen\",\n",
    "        \"POS\": \"VERB\",\n",
    "        \"Gram\": \"VerbForm=Inf\",\n",
    "        \"Syntax\": \"xcomp\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \"לפגוש\",\n",
    "            \"lemma\": \"לפגוש\",\n",
    "            \"POS\": \"VERB\",\n",
    "            \"Gram\": \"HebBinyan=HIFIL|VerbForm=Inf|Voice=Act\",\n",
    "            \"Syntax\": \"xcomp\"\n",
    "        }\n",
    "    },\n",
    "    \".\": {\n",
    "        \"lemma\": \".\",\n",
    "        \"POS\": \"PUNCT\",\n",
    "        \"Gram\": \"None\",\n",
    "        \"Syntax\": \"punct\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": \".\",\n",
    "            \"lemma\": \".\",\n",
    "            \"POS\": \"PUNCT\",\n",
    "            \"Gram\": \"None\",\n",
    "            \"Syntax\": \"punct\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку морфология и синтаксис в иврите и немецком различаются, формирование словаря должно проводиться с опорой на следующие правила:\n",
    "\n",
    "1. Проверяем часть речи: если одинаковая в обеих языках, сравниваем грамматические признаки; если разная, ищем другую лексему с совпадающей частеречной категорией. Добавляем в список возможных кандидатов все лексемы в тексте с их леммами и морфосинтаксическими характеристиками.\n",
    "\n",
    "2. Проверка частей речи завершена - сравниваем грамматические признаки (например, для глагола): убираем те ивритские лексемы, лицо и число которых не совпадает с лицом и числом немецкой словоформы. Оставляем только совпадающие. В словарь ивритского аналога добавляем все возможные значения категории Gender. Так, например, для глагола heißen во вложенном словаре со значениями для иврита должно содержаться две лексемы - для мужского и женского родов. Соответственно, в грамматических категориях это тоже должно быть отражено (в идеале, должны быть учтены все омонимичные формы, поскольку глагол heißen в настоящем времени имеет одинаковую форму для второго и третьего лица единственного числа, а в иврите в настоящем времени формы глагола не дифференцируются по лицу: только по числу и роду). Таким образом, словарь для формы 'heißt' будет выглядеть следующим образом:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = {\"heißt\": {\n",
    "        \"lemma\": \"heißen\",\n",
    "        \"POS\": \"VERB\",\n",
    "        \"Gram\": [\"Mood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin\", \n",
    "                 \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\"],\n",
    "        \"Syntax\": \"root\",\n",
    "        \"hebrew\": {\n",
    "            \"word\": [\"קוֹרֵא\", \"קוֹרֵאת\"],\n",
    "            \"lemma\": \"קרא\", \n",
    "            \"POS\": \"VERB\",\n",
    "            \"Gram\": [\"Gender=Masc|HebBinyan=PAAL|Number=Sing|Tense=Pres|VerbForm=Part|Voice=Act\", \n",
    "                     \"Gender=Fem|HebBinyan=PAAL|Number=Plur|Tense=Pres|VerbForm=Part|Voice=Act\"\n",
    "                     ],\n",
    "            \"Syntax\": \"root\"\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое касается местоимений второго лица: во вложенном словаре местоимениq du и ihr для иврита должны содержаться эквиваленты в мужском и женском роде, которые в тексте перевода должны быть выведены через слэш с соответствующиими пометками (m.) и (f.)\n",
    "\n",
    "3. Если на расстоянии (3, 3) есть слово, грамматический род которого известен, и синтаксическая роль указывает на то, что оно связано с местоимением второго лица, из словаря при переводе должен быть выбран эквивалент, род которого соответствует роду маркированного слова, по которому опознается род местоимения.\n",
    "\n",
    "4. После формирования списка лексем для вывода им должны быть присвоены индексы. Поскольку слова в иврите читаются справа налево, общее правило вывода текста перевода заключается в зеркальном позиционном кодировании: токен с индексом 0 в немецком соответствует токену с индексом -1 в иврите, первый токен в немецком тексте = минус второй токен в тексте на иврите, и т. д.\n",
    "\n",
    "5. При переводе местоименных посессивных конструкций с генитивным аргументом (такие конструкции отслеживаем по значению ключа Syntax в словарях) вершина и зависимое должны меняться местами: в немецком языке порядок зависимое-вершина (meine Mutter), а в иврите - вершина-зависимое (/има (Mutter) шели (meine)/). \n",
    "\n",
    "На самом деле, правил для этой языковой пары должно быть очень много, но если перечислять все, то можно состариться... Хорошо, что придумали статистический и нейросетевой перевод.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
