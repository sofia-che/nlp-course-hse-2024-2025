{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Статистический машинный перевод** - вид машинного перевода, где эквивалент в языке-реципиенте подбирается на основании вероятности соответствия определенному токену в языке-источнике.\n",
    "\n",
    "Формальное определение работы статистической модели основано на теореме Байеса:\n",
    "\n",
    "P(Y|X) = (P(Y)*P(X|Y)) / P(X), где:\n",
    "\n",
    "P(Y) - *априорная* вероятность (вероятность наступления события без учёта других событий)\n",
    "\n",
    "P(X|Y) - *апостериорная* вероятность (вероятность наступления события с учётом того факта, что другое событие уже произошло)\n",
    "\n",
    "В этом случае Y - единица на языке перевода, а X - единица на языке оригинала. \n",
    "\n",
    "Выделяют СМТ, основанный на:\n",
    "1. Слове\n",
    "2. Сегменте\n",
    "3. Синтаксисе\n",
    "4. Фразе (иерархический)\n",
    "\n",
    "Принцип работы СМТ:\n",
    "1. В начале собираются большие параллельные корпуса текстов, для которых мы хотим обучить модель, в качестве обучающей и тестовой выборок\n",
    "2. Тексты предобрабатываются (токенизируются) и объединяются в последовательности в зависимости от того, на какой последовательности мы хотим обучать модель: если мы, например, имеем дело с биграммной моделью, токены объединяются в пары: i + i+1\n",
    "3. Для последовательностей вычисляется вероятность совместной встречаемости в зависимости от того, насколько часто такие комбинации появляются в корпусах\n",
    "4. Из набора токенов, имеющих ненулевую вероятность появления после токена i, выбирается тот, у которого вероятность наибольшая, благодаря чему осуществляется поиск наиболее вероятных комбинаций.\n",
    "5. В зависимости от грамматики языков, на которых мы обучаем модель, выбираем наиболее корректный вариант перевода с точки зрения морфосинтаксической структуры, в том числе порядка слов. Этот процесс подбора называется *декодированием*.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведён игрушечный пример обучения статистической модели на примере немецко-английского параллельного корпуса. Следует отметить, что качество обучения у модели нулевое, поскольку данных в корпусе слишком мало."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open('de-en.tgz', 'r:gz') as tar:\n",
    "  tar.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('de-en.de', 'r', encoding='utf8') as f:\n",
    "  german = f.read().split('\\n')[:-1]\n",
    "\n",
    "with open('de-en.en', 'r', encoding='utf8') as f:\n",
    "  english = f.read().split('\\n')[:-1]\n",
    "\n",
    "print(\"Данные языка X:\\n\", german)\n",
    "print(\"Данные языка Y:\\n\", english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(english, german)\n",
    "\n",
    "print(\"> Обучающая выборка:\")\n",
    "for text, label in zip(X_train, y_train):\n",
    "    print(f\"\\nТекст на немецком: {label}\\n Его перевод на английский: {text}\\n\")\n",
    "\n",
    "print(\"> Тестовая выборка:\")\n",
    "for text, label in zip(X_test, y_test):\n",
    "    print(f\"\\nТекст на немецком: {label}\\n Его перевод на английский: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "  # функция возвращает списки слов\n",
    "  return [sentence.split() for sentence in sentences]\n",
    "\n",
    "# токенизируем каждую выборку\n",
    "X_train_tokens, X_test_tokens, y_train_tokens, y_test_tokens = tokenize(X_train), tokenize(X_test), tokenize(y_train), tokenize(y_test)\n",
    "\n",
    "print('Образец токенизированного текста:', X_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vocab = Counter(' '.join(german).split()).keys()\n",
    "y_vocab = Counter(' '.join(english).split()).keys()\n",
    "\n",
    "print(f\"Словарь немецких словоформ: {x_vocab}\\n Всего {len(x_vocab)} словоформ\")\n",
    "print(f\"\\nCловарь английских словоформ: {y_vocab}\\n Всего {len(y_vocab)} словоформ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вероятность того, что случайное слово x_vocab соответсвует случайному слову y_vocab\n",
    "uniform = 1 / (len(x_vocab) * len(y_vocab))\n",
    "result = format(uniform, '.10f') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-model\n",
    "t = {}\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "  # начинаем итерацию по обучающей выборке\n",
    "  for word_x in X_train_tokens[i]:\n",
    "    for word_y in y_train_tokens[i]:\n",
    "      # создаем t-table\n",
    "      t[(word_x, word_y)] = uniform\n",
    "\n",
    "# t-table\n",
    "for elem in t:\n",
    "  print(\"Соответствие |\", elem[0], \"  ->  \", elem[1], \"| Вероятность:\", format(t[elem], '.10f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# количество итераций обучения\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  # начинаем обучение\n",
    "\n",
    "  # шаг 0. создаем слоты для подсчета статистики\n",
    "  count = {} # P(x|y)\n",
    "  total = {} # P(y)\n",
    "\n",
    "  for i in range(len(X_train)):\n",
    "    # начинаем итерацию по обучающей выборке\n",
    "    for word_x in X_train_tokens[i]:\n",
    "      for word_y in y_train_tokens[i]:\n",
    "        # создаем слоты для подсчета условной вероятности совпадений в корпусе\n",
    "        count[(word_x, word_y)] = 0\n",
    "        # и слоты для статистической языковой модели y\n",
    "        total[word_y] = 0\n",
    "\n",
    "  # шаг 1. Expectation\n",
    "  for i in range(len(X_train)):\n",
    "    # начинаем итерацию по обучающей выборке\n",
    "    total_stat = {} # статистика x\n",
    "\n",
    "    # собираем предварительную статистику на основе данных x\n",
    "    for word_x in X_train_tokens[i]:\n",
    "      total_stat[word_x] = 0 # создаем слоты для подсчета статистики по каждому токену x\n",
    "      for word_y in y_train_tokens[i]:\n",
    "        # обновляем данные из t-table; увеличиваем значения при обнаружении совместной встречаемости\n",
    "        total_stat[word_x] += t[(word_x, word_y)]\n",
    "\n",
    "    # обновляем данные для P(x|y) и P(y)\n",
    "    for word_x in X_train_tokens[i]:\n",
    "      for word_y in y_train_tokens[i]:\n",
    "        # подсчет условной вероятности совпадений в корпусе: равномерное распределение / частотность x\n",
    "        count[(word_x, word_y)] += t[(word_x, word_y)] / total_stat[word_x]\n",
    "        # подсчет статистической информации y: равномерное распределение / частотность x\n",
    "        total[word_y] += t[(word_x, word_y)] / total_stat[word_x]\n",
    "\n",
    "  # шаг 2. Maximization\n",
    "  for i in range(len(X_train)):\n",
    "    # начинаем итерацию по обучающей выборке\n",
    "    for word_x in X_train_tokens[i]:\n",
    "      for word_y in y_train_tokens[i]:\n",
    "        # обновляем t-table: вероятность совпадения в корпусе / вероятность информации y\n",
    "        t[(word_x, word_y)] = count[(word_x, word_y)] / total[word_y]\n",
    "\n",
    "for elem in t:\n",
    "  print(\"Соответствие |\", elem[0], \"  ->  \", elem[1], \"| Вероятность:\", round(t[elem], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для обучения модели объединим 2 выборки\n",
    "tokens = ' '.join(german).split()\n",
    "\n",
    "# хранилище для биграмм\n",
    "bigram_model = defaultdict(list)\n",
    "\n",
    "# собираем все попарные совпадения\n",
    "for i in range(len(tokens)-1):\n",
    "    current_word = tokens[i]\n",
    "    next_word = tokens[i + 1]\n",
    "    bigram_model[current_word].append(next_word)\n",
    "\n",
    "print(bigram_model)\n",
    "\n",
    "def decoder(model, steps=5):\n",
    "  # инициализация случайного токена\n",
    "  current_word = random.choice(tokens)\n",
    "  generated_sentence = current_word\n",
    "\n",
    "  for step in range(steps):\n",
    "    # пошаговая генерация\n",
    "    print('Шаг', step+1)\n",
    "    next_word_options = model[current_word]\n",
    "    print(f'Правдоподобные варианты продолжения для токена {current_word}:', next_word_options)\n",
    "\n",
    "    current_word = random.choice(next_word_options)\n",
    "    generated_sentence += ' '\n",
    "    generated_sentence += current_word\n",
    "    print('Промежуточный результат:', generated_sentence)\n",
    "    print()\n",
    "  print('Результат:', generated_sentence)\n",
    "\n",
    "decoder(bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сортировка t-table по убыванию правдоподобия\n",
    "sorted_t = sorted(t.items(), key = lambda k:(k[1], k[0]), reverse = True)\n",
    "\n",
    "def translate(token):\n",
    "  for element in sorted_t:\n",
    "    if element[0][1] == token:\n",
    "      # поиск совпадений в t-table\n",
    "      return element[0][0]\n",
    "    # если совпадений нет, функция будет возвращать None\n",
    "    return None \n",
    "\n",
    "for sentence in y_test_tokens:\n",
    "  print(\"Оригинальное предложение:\", ' '.join(sentence))\n",
    "  translation = list(map(lambda token: translate(token) or '', sentence)) # обрабатываем все случаи, включая те, когда совпадений в таблице найдено не было\n",
    "  print(\"Перевод:\", ' '.join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu \n",
    " \n",
    "reference = [X_test_tokens[8]]  \n",
    " \n",
    "candidate = [translate(token) for token in y_test_tokens[0]] \n",
    " \n",
    "bleu_score = corpus_bleu([reference], [candidate])  \n",
    " \n",
    "print(\"BLEU Score:\", format(bleu_score, '.10f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
